{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scipy_Pubg_Intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "pYGbXHMlvllq"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gHHSLFS8vlls",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "91n-ulONvllw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cqnr7eHHvll1"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-TyG9xF0vll1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vpVczftevll4"
      },
      "cell_type": "markdown",
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b3DzsEvyvll6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NAqhbJCiYHu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Big-Data**\n",
        "Big data is a term that describes the large volume of data – both structured and unstructured\n",
        "Big data can be analyzed for insights that lead to better decisions and strategic business moves.\n",
        "\n",
        "### **Why big data?**\n",
        "The importance of big data doesn’t revolve around how much data you have, but what you do with it. You can take data from any source and analyze it to find answers that enable 1) cost reductions, 2) time reductions, 3) new product development and optimized offerings, and 4) smart decision making. When you combine big data with high-powered analytics, you can accomplish business-related tasks such as:\n",
        "Determining root causes of failures, issues and defects in near-real time.\n",
        "Generating coupons at the point of sale based on the customer’s buying habits.\n",
        "Recalculating entire risk portfolios in minutes.\n",
        "Detecting fraudulent behavior before it affects your organization.\n",
        "\n",
        "### Three V of big data:\n",
        "\n",
        "Volume: The amount of data is immense.  Each day 2.3 trillion gigabytes of new data is being created.\n",
        "\n",
        "Velocity: The speed of data (always in flux) and processing (analysis of streaming data to produce near or real time results)\n",
        "\n",
        "Variety:  The different types of data, structured, as well as, unstructured.\n"
      ]
    },
    {
      "metadata": {
        "id": "zzwm8pR-WhwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HDFS Architecture\n",
        "\n",
        "![alt text](https://github.com/Heisenberg0203/PUBG/blob/master/images/HDFS%20Architecture.png?raw=true)\n",
        "\n",
        "**HDFS** :- Hadoop Distributed File System\n",
        "Smallest Unit of Storage in filesystem is block. HDFS has *block size of 128MB* (default) which is configurable unlike traditional OS where blocksize is of 4 kB. It splits the large file accordingly and place every block in different node i.e it is* distributed*. Second, Each block by default has 3 replicas stored in nodes(i.e it has *reliability* and it is known as replication factor.By default it is 3 and yes, it is configurable).Each replica is stored in different node, so that even if one node gets failed, block can be accessed from another(Fault Tolerant).\n",
        "\n",
        "There are types of node in HDFS:-\n",
        "\n",
        "**Datanode** :- This is the daemon that runs on Slave node, these are the ones that actually stores the data.\n",
        "\n",
        "**Namenode**: - This is the master node that runs all the datanode. It has all the metadata information like which block of file is present on which node, replication factor, block size, faulty nodes etc etc."
      ]
    },
    {
      "metadata": {
        "id": "Op_q6_PudMCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###What exactly is Spark and why is its need? :-\n",
        "Traditional Processing was unable to process huge amount of data(Big Data) i.e being generated, so there was need of different platform different file structure, a way completely new technique of processing and here comes Apache Spark Framework.\n"
      ]
    },
    {
      "metadata": {
        "id": "KZeQHNxNdTqe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Spark:\n",
        "Apache Spark is an open-source distributed general-purpose cluster-computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
        "\n",
        "*   **Distributed**:  A distributed system is a system whose components are located on different networked computers, which then communicate and coordinate their actions by passing messages to one another. \n",
        "*   **General purpose** : broadly applicable across application domains, and lacks specialized features for a particular domain.\n",
        "*   **Open Source**: Open-source software is a type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose.\n",
        "*  **Cluster** : A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "X81FPL0hddVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Spark's Architecture:\n",
        "Spark uses a master/worker architecture. There is a driver that talks to a single coordinator called master that manages workers in which executors run.\n",
        "![alt text](https://github.com/purva11198/PUBG/blob/master/images/Spark%20Architecture.jpg?raw=true)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ophorxmccDeU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Use Spark!\n",
        "That's all there is to it - you're ready to use Spark!"
      ]
    },
    {
      "metadata": {
        "id": "byqVgBvfxFq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrnck1RCxOtb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"Scipy\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khWTApfYdu_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Resilient Distributed Dataset :- \n",
        "RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
        "\n",
        "Each dataset in RDD is divided into logical partitions which can be computed on different nodes of cluster.\n",
        "\n",
        "There are 2 ways to create RDD:-\n",
        "\n",
        "*   By parallelizing existing collection\n",
        "*   By reading a dataset present in file,table ,hive or any other data source.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AuL6ROHtxaVA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5,6,7,8,9,10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjMhj2Kfxju5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For creating RDD, which is nothing but distributed dataset, you can create it by using sc.parallelize\n",
        "distData = sc.parallelize(data) \n",
        "#here number of partitions are equal to number of clusters, you can manually create number of partitions by  \n",
        "#distData = sc.parallelize(data,num_of_partitions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2nrvAmQoeOZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Lets Checkout type of our distibuted dataset"
      ]
    },
    {
      "metadata": {
        "id": "X-sG5WCRxqk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(distData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7r0R0HSeoKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are 2 things we can do on RDD's \n",
        "\n",
        "*   Transaformations\n",
        "*   Actions\n",
        "\n",
        "##Unless and until Action is not called, Transformations doesn't happens, due to its lazy architecuture.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OXDiA89FeTEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformation\n",
        "![alt text](https://github.com/purva11198/PUBG/blob/master/images/RDD%20Transformation.jpg?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "feac24BoeXAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The RDD can be Seen by calling Collect Action on it"
      ]
    },
    {
      "metadata": {
        "id": "YuQvGxgFxut6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "distData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TXWlGY7e5Fp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation :Filter "
      ]
    },
    {
      "metadata": {
        "id": "SmhJuSG8x5QQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filterData = distData.filter(lambda x: x<5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYm5pZ7qyGmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filterData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VtzPIKXyI9h",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "#Find elements in datasets that are divisible by 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-bHFlUThWl5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : map"
      ]
    },
    {
      "metadata": {
        "id": "diOkmJhFycZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapData = distData.map(lambda x : x*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLu0gsi2yk10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_i95nGMynlb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "#Take square of each number and add 5 to it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ssPbhSDzhZqO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : mapToPair"
      ]
    },
    {
      "metadata": {
        "id": "-VvwO4Y1y1-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData = distData.map(lambda x:(x,x+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsgbvKi8zCTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2nAGPxSzE-W",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "#Find the pair of number and its cube"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yABuD9kGzcL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textdata = [\"hello world i am learning apache Spark and i am coding at Scipy 2018 \"]\n",
        "textData =sc.parallelize(textdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w7iAi7vgzxXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2pCQRquzhhSA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : flatmap"
      ]
    },
    {
      "metadata": {
        "id": "KwtnOKclzzmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flatmap =textData.flatMap(lambda x : x.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgN7U_pO0av8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flatmap.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrdAmtL3hkWh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : GroupBy/GroupByKey\n"
      ]
    },
    {
      "metadata": {
        "id": "DsyRHO4X0kvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data =['hello','hello','world']\n",
        "textData  = sc.parallelize(data)\n",
        "groupData = textData.groupBy(lambda x:x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QGaKXqQ2nhS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for group in groupData.collect():\n",
        "  print(group[0])\n",
        "  print(group[1])\n",
        "  for i in group[1]:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3hw3enlV0uXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "groupData.mapValues(len).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMz7gqVc6TLb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "##group the dataset by 1 charachter, and then print its count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hK7j4CzC6ocS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data =['James','Thomas','Arya','Ned','Jamie','Jon','Maradona','James','Ned',]\n",
        "textData  = sc.parallelize(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NpS4oyZF7MA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData = textData.map(lambda x:(x,1))\n",
        "mapToPairData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ld5kX1HWhp4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : reduceByKey"
      ]
    },
    {
      "metadata": {
        "id": "8IA3BJ117PiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData.reduceByKey(lambda x,y : x+y).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpatdQchhtkq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation : Sort/SortByKey"
      ]
    },
    {
      "metadata": {
        "id": "N_WBnpVk8Mm9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData.sortByKey(ascending=False).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEy5qTq4AJwI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Persist\n",
        "Set this RDD’s storage level to persist its values across operations after the first time it is computed. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. If no storage level is specified defaults to (MEMORY_ONLY)."
      ]
    },
    {
      "metadata": {
        "id": "fNEHCgjG8upg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapToPairData.persist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-08VU7lB_NLf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Wordcount"
      ]
    },
    {
      "metadata": {
        "id": "cPwsrNUyDe-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ['A wonderful king is Hadoop. The elephant plays well with Sqoop. But what helps him to thrive Are Impala, and Hive,And HDFS in the group.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.']\n",
        "##make a distributed dataset\n",
        "textData = \n",
        "##split the dataset into words\n",
        "words = \n",
        "##generate key value pairs (word,1)\n",
        "wordsAndOnes =\n",
        "##generate count of word\n",
        "##method 1\n",
        "groupWords =\n",
        "output= \n",
        "##show output\n",
        "output.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S45ewhut_ROf",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution - Way 1 { vertical-output: true }\n",
        "data = ['A wonderful king is Hadoop. The elephant plays well with Sqoop. But what helps him to thrive Are Impala, and Hive,And HDFS in the group.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.']\n",
        "textData = sc.parallelize(data)\n",
        "##split the dataset into words\n",
        "words = textData.flatMap(lambda x : x.split())\n",
        "##generate key value pairs (word,1)\n",
        "wordsAndOnes = words.map(lambda word:(word,1))\n",
        "##generate count of word\n",
        "##method 1\n",
        "groupWords = wordsAndOnes.groupByKey()\n",
        "output=groupWords.mapValues(len)\n",
        "output.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59BDYWO-A1ZE",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution - Way 2 { vertical-output: true }\n",
        "data = ['A wonderful king is Hadoop. The elephant plays well with Sqoop. But what helps him to thrive Are Impala, and Hive,And HDFS in the group.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.A wonderful king is Hadoop. The elephant plays well with Sqoop.']\n",
        "textData = sc.parallelize(data)\n",
        "##split the dataset into words\n",
        "words = textData.flatMap(lambda x : x.split())\n",
        "##generate key value pairs (word,1)\n",
        "wordsAndOnes = words.map(lambda word:(word,1))\n",
        "##generate count of word\n",
        "##method 1\n",
        "output = wordsAndOnes.reduceByKey(lambda x,y : x+y)\n",
        "output.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ikgz8-g_A-Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sortedWords=output.sortByKey()\n",
        "sortedWords.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVo-Y7HlBBBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sortedCount = output.sortBy(lambda x:x[1])\n",
        "sortedCount.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}